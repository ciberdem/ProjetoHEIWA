#Pipeline de Limpeza de Dados v3#
Este reposit√≥rio cont√©m um pipeline de limpeza de dados v3 em Python, desenvolvido para pr√©-processar conjuntos de dados textuais. O pipeline foi desenvolvido para limpeza de dados advindos de redes sociais. Entretanto, ele pode ser utilizado para v√°rias bases de dados textuais. O pipeline utiliza diversas bibliotecas, ferramentas e t√©cnicas para garantir a qualidade e a consist√™ncia dos dados, incluindo agora tokeniza√ß√£o e remo√ß√£o de stopwords com NLTK.

<img src="https://github.com/ciberdem/ProjetoHEIWA-FAPESP/blob/main/CuradoriaExtracaoDados/pipeline_v2/assets/Pipeline_diagrama.png" alt="Diagrama do pipeline de limpeza">

Conte√∫do
Como Utilizar

Depend√™ncias

Instala√ß√£o das Depend√™ncias

Executando o Pipeline

Exemplo de Uso

Funcionalidades

Par√¢metros para Ativar/Desativar Etapas

Fun√ß√µes do Pipeline

Estrutura de Sa√≠da

Detalhes do Pr√©-processamento de Texto

Como Utilizar
Depend√™ncias
Antes de executar o pipeline, certifique-se de ter as seguintes bibliotecas instaladas:

pandas : https://pandas.pydata.org/

demoji : https://pypi.org/project/demoji/

enelvo : https://github.com/thalesbertaglia/enelvo

unidecode : https://pypi.org/project/Unidecode/

nltk : https://www.nltk.org/

Instala√ß√£o das Depend√™ncias
Para instalar todas as depend√™ncias, utilize o arquivo requirements.txt. Com o arquivo pronto, basta executar o comando:

Bash

pip install -r /path/to/requirements.txt
Voc√™ tamb√©m precisar√° baixar os recursos necess√°rios do NLTK (stopwords e tokenizador):

Python

import nltk
nltk.download('stopwords')
nltk.download('punkt')
Executando o Pipeline
Arquivo de entrada: O pipeline aceita arquivos nos formatos .csv ou .json. O caminho do arquivo de entrada e o nome da coluna que cont√©m o texto a ser processado devem ser fornecidos.

Processamento: O pipeline pode ser executado por meio da fun√ß√£o pipeline. Ela carrega o arquivo, aplica o pr√©-processamento e exporta o resultado.

OBS: Todas as etapas de pr√©-processamento s√£o opcionais e podem ser ativadas ou desativadas conforme necess√°rio ao chamar a fun√ß√£o pipeline. Cada etapa √© controlada por um par√¢metro booleano (True para ativar e False para desativar).

Exemplo de Uso
Python

from pipeline import pipeline

caminho_arquivo = 'dados.csv'  # ou 'dados.json'
coluna = 'texto'
formato = 'json'

# Executando o pipeline com todas as etapas padr√£o (incluindo tokeniza√ß√£o)
pipeline(caminho_arquivo, coluna, formato)
Funcionalidades
O pr√©-processamento de texto desempenha um papel crucial na qualidade e na consist√™ncia dos dados. No pipeline v3, v√°rias etapas s√£o realizadas para garantir que o texto de entrada seja limpo e adequado para an√°lises subsequentes, como:

Remo√ß√£o de men√ß√µes a usu√°rios (@usuario).

Remo√ß√£o de URLs.

Normaliza√ß√£o Enelvo (e tratamento de v√≠rgulas).

Substitui√ß√£o de emojis por r√≥tulos sentimentais.

Convers√£o de caracteres especiais para ASCII.

Remo√ß√£o de pontua√ß√µes e caracteres especiais.

(Novo) Tokeniza√ß√£o do texto e remo√ß√£o de stopwords.

Para desativar etapas espec√≠ficas, basta definir o par√¢metro desejado como False ao chamar a fun√ß√£o pipeline. Por exemplo, para desativar a normaliza√ß√£o e a nova etapa de tokeniza√ß√£o:

Python

from pipeline import pipeline

# Executa o pipeline sem normalizar e sem tokenizar
pipeline('dados.csv', 'texto', formato='json', normalizar=False, tokenizar_texto=False)
Par√¢metros para Ativar/Desativar Etapas
normalizar (padr√£o: True): Aplica a normaliza√ß√£o ao texto para padronizar varia√ß√µes informais utilizando ferramenta enelvo.

substituir_emojis (padr√£o: True): Substitui emojis por r√≥tulos sentimentais.

substituir_users (padr√£o: True): Remove men√ß√µes a usu√°rios (exemplo: @usuario).

remover_urls (padr√£o: True): Remove URLs do texto.

converter_ascii (padr√£o: True): Converte caracteres especiais para ASCII.

remover_pontuacao (padr√£o: True): Remove pontua√ß√µes n√£o associadas a n√∫meros.

tokenizar_texto (padr√£o: True): (Novo) Tokeniza o texto, remove stopwords e tokens n√£o-alfab√©ticos.

Fun√ß√µes do Pipeline
1. substitui_emoji(text)
Substitui os emojis por r√≥tulos sentimentais (emojipositivo, emojinegativo, emojineutro) ou descri√ß√µes Unicode.

Par√¢metros: text (str): Texto no qual os emojis ser√£o substitu√≠dos.

Retorno: Texto com emojis substitu√≠dos.

Python

def substitui_emoji(text):
    """
    Substitui emojis em um texto por r√≥tulos de sentimento ou descri√ß√µes de emojis.

    Par√¢metros:
    text (str): O texto no qual os emojis ser√£o substitu√≠dos.

    Retorna:
    str: O texto com os emojis substitu√≠dos por r√≥tulos de sentimentos ou descri√ß√µes Unicode.
    """
     
    for emoji, label in emoji_list.items():
        text = text.replace(emoji, label)
    dem = demoji.findall(text)
    for item, value in dem.items():
        text = text.replace(item, f" {value.replace(' ', '')}")
    return text
2. tokenizar_e_limpar_texto(text) (Nova Fun√ß√£o)
Remove as stopwords presentes no texto e filtra tokens n√£o-alfab√©ticos, utilizando a biblioteca NLTK.

Par√¢metros: text (str): O texto no qual as stopwords ser√£o removidas.

Retorno: list: Uma lista de tokens (palavras) filtrados.

Python

def tokenizar_e_limpar_texto(text):
  """
  Remove as stopwords presente no texto, atrav√©s da biblioteca NLTK

  Par√¢metros:
  text (str): O texto no qual as stopwords ser√£o removidas.

  Retorna:
  str: O texto sem as stop words previamente removidas.
  """
  text = text.lower()
  tokens = word_tokenize(text, langauge='portuguese') # Nota: 'langauge' √© um typo no c√≥digo original
  tokens_filtrados = [
    palavra for palavra in tokens
    if palavra.isalpha() and palavra.lower() not in stopwords_pt
  ]

  return tokens_filtrados
3. preprocess(texto, normalizar=True, substituir_emojis=True, ...)
Aplica o pipeline de pr√©-processamento ao texto com diversas etapas opcionais, agora incluindo tokenizar_texto.

Par√¢metros:

texto (pd.Series): S√©rie Pandas contendo os textos a serem processados.

normalizar (bool): Substitui v√≠rgulas temporariamente e normaliza o texto com a ferramenta enelvo.

substituir_emojis (bool): Substitui emojis por r√≥tulos sentimentais.

substituir_users (bool): Remove men√ß√µes a usu√°rios.

remover_urls (bool): Remove URLs do texto.

converter_ascii (bool): Converte caracteres especiais para ASCII.

remover_pontuacao (bool): Remove pontua√ß√µes n√£o associadas a n√∫meros.

tokenizar_texto (bool): (Novo) Tokeniza o texto e remove stopwords.

Retorno: S√©rie Pandas com o texto pr√©-processado.

Python

def preprocess(texto, normalizar=True, substituir_emojis=True, substituir_users=True, remover_urls=True, converter_ascii=True, remover_pontuacao=True, tokenizar_texto=True):
    """
    Aplica o pipeline de pr√©-processamento ao texto fornecido.

    Par√¢metros:
    texto (pd.Series): Uma s√©rie pandas contendo os textos a serem processados.
    normalizar (bool): Normaliza o texto usando o enelvo. Padr√£o: True.
    substituir_emojis (bool): Substitui emojis por r√≥tulos de sentimentos. Padr√£o: True.
    substituir_users (bool): Remove men√ß√µes a usu√°rios no formato @usuario. Padr√£o: True.
    remover_urls (bool): Remove URLs do texto. Padr√£o: True.
    converter_ascii (bool): Converte caracteres especiais para ASCII. Padr√£o: True.
    remover_pontuacao (bool): Remove pontua√ß√µes desnecess√°rias. Padr√£o: True.
    tokenizar_texto (bool): Tokeniza e remove stopwords. Padr√£o: True.

    Retorna:
    pd.Series: Uma s√©rie pandas com o texto pr√©-processado.
    """

    # Substituindo users
    if substituir_users:
        texto = texto.str.replace(r'@\w+', '')

    # Removendo URLs
    if remover_urls:
        texto = texto.apply(lambda x: re.sub(r'http\S+', '', x))

    # Substituindo v√≠rgulas por "chavevirg"
    if normalizar:
        texto = texto.str.replace(r',', 'chavevirg')

    # Normalizando texto (enelvo)
    if normalizar:
        texto = texto.apply(lambda x: normalizador.normalise(x))

    # Substituindo emojis
    if substituir_emojis:
        texto = texto.apply(substitui_emoji)

    # Revertendo substitui√ß√£o de v√≠rgulas
    if normalizar:
        texto = texto.str.replace(r'chavevirg', ',')

    # Convertendo para ASCII
    if converter_ascii:
        texto = texto.apply(lambda x: unidecode(x))

    # Removendo v√≠rgulas n√£o associadas a n√∫meros e outras pontua√ß√µes
    if remover_pontuacao:
        texto = texto.apply(lambda x: re.sub(r'(?<!\d),(?=\D)|(?<=\D),(?!\d)|(?<!\d),(?=\d)|(?<!\d)\/|\/(?!\d)|_|[^\w#\/\s,\@]','', x))

    if tokenizar_texto:
        texto = texto.apply(tokenizar_e_limpar_texto)

    return texto
4. pipeline_export(df, coluna, formato='json', **kwargs)
Aplica o pr√©-processamento em uma coluna de um DataFrame e exporta os dados processados.

Par√¢metros:

df (pd.DataFrame): DataFrame original contendo os textos.

coluna (str): Nome da coluna que ser√° processada.

formato (str): Formato de exporta√ß√£o (json ou csv).

**kwargs: Par√¢metros opcionais para as etapas de pr√©-processamento.

Retorno:

DataFrame original com uma nova coluna texto_preprocessado.

Python

def pipeline_export(df, coluna, formato='json', **kwargs):
    """
    Aplica o pr√©-processamento em uma coluna do DataFrame e exporta o DataFrame modificado.

    Par√¢metros:
    df (pd.DataFrame): O DataFrame original contendo os dados.
    coluna (str): Nome da coluna que ser√° processada.
    formato (str): Formato de exporta√ß√£o dos dados ('json' ou 'csv'). Padr√£o: 'json'.
    **kwargs: Par√¢metros opcionais para as etapas do pr√©-processamento.

    Retorna:
    pd.DataFrame: O DataFrame original com uma nova coluna chamada 'texto_preprocessado'.
    """

    df['texto_preprocessado'] = preprocess(df[coluna], **kwargs)

    # Exportar para o formato escolhido
    if formato == 'json':
        df.to_json('saida_preprocessada.json', orient='records', lines=True, force_ascii=False)
    elif formato == 'csv':
        df.to_csv('saida_preprocessada.csv', index=False)
    else:
        print(f"Formato {formato} n√£o √© suportado.")
    
    return df  # Retorna o DataFrame original com a nova coluna
5. pipeline(caminho_arquivo, coluna, formato='csv', **kwargs)
Fun√ß√£o principal que aplica o pipeline ao arquivo fornecido e exporta o resultado.

Par√¢metros:

caminho_arquivo (str): Caminho do arquivo a ser processado (.csv ou .json).

coluna (str): Nome da coluna a ser processada.

formato (str): Formato de exporta√ß√£o (csv ou json).

**kwargs: Par√¢metros opcionais para as etapas de pr√©-processamento.

Retorno:

Exibe o DataFrame modificado ap√≥s o processamento e exporta√ß√£o.

Python

def pipeline(caminho_arquivo, coluna, formato='csv', **kwargs):
    """
    Fun√ß√£o principal que aplica o pipeline de pr√©-processamento ao arquivo fornecido.

    Par√¢metros:
    caminho_arquivo (str): Caminho para o arquivo a ser processado (.csv ou .json).
    coluna (str): Nome da coluna que ser√° processada no DataFrame.
    formato (str): Formato de exporta√ß√£o dos dados ('json' ou 'csv'). Padr√£o: 'csv'.
    **kwargs: Par√¢metros opcionais para as etapas do pr√©-processamento.

    Retorna:
    None: Exibe o DataFrame modificado ap√≥s o processamento e exporta√ß√£o.
    """

    # Verificar a extens√£o do arquivo para carregar o DataFrame corretamente
    if caminho_arquivo.endswith('.csv'):
        df = pd.read_csv(caminho_arquivo)
    elif caminho_arquivo.endswith('.json'):
        df = pd.read_json(caminho_arquivo, lines=True)
    else:
        raise ValueError("Formato de arquivo n√£o suportado. Use um arquivo .csv ou .json")
    
    # Aplicar o pipeline e exportar
    df_modificado = pipeline_export(df, coluna, formato, **kwargs)
    
    # Exibir o DataFrame modificado
    print(df_modificado)
Estrutura de Sa√≠da
O pipeline cria uma nova coluna chamada texto_preprocessado no DataFrame original e exporta o resultado em um dos seguintes formatos:

JSON

CSV

Para escolher o formato de sa√≠da, utilize o par√¢metro formato ao chamar a fun√ß√£o pipeline ou pipeline_export. Defina formato='json' para exportar como JSON ou formato='csv' para CSV.

Python

from pipeline import pipeline

# Executa o pipeline e exporta o resultado em formato CSV
pipeline('dados.csv', 'texto', formato='csv')
Detalhes do Pr√©-processamento de Texto
(Ordem atualizada para v3)

1. Remo√ß√£o de Usu√°rios
O c√≥digo realiza a remo√ß√£o de men√ß√µes a usu√°rios no formato @usu√°rio.

Python

texto = texto.str.replace(r'@\w+', '')
2. Remo√ß√£o de URLs
Qualquer URL presente no texto √© removida usando uma express√£o regular que identifica padr√µes de URLs, come√ßando com "http".

Python

texto = texto.apply(lambda x: re.sub(r'http\S+', '', x))
3. Substitui√ß√£o de V√≠rgulas
Nesta etapa, todas as v√≠rgulas no texto s√£o temporariamente substitu√≠das por "chavevirg". Isso √© feito para contornar a ferramenta Enelvo, que separa n√∫meros com v√≠rgula durante a normaliza√ß√£o. A substitui√ß√£o tempor√°ria facilita a manuten√ß√£o da integridade dos dados num√©ricos e √© revertida posteriormente.

Python

texto = texto.str.replace(r',', 'chavevirg')
4. Normaliza√ß√£o Enelvo
O pr√≥ximo passo √© a utiliza√ß√£o da biblioteca Enelvo, que envolve a normaliza√ß√£o de erros ortogr√°ficos, g√≠rias da internet, siglas, nomes pr√≥prios e outros.

Python

texto = texto.apply(lambda x: normalizador.normalise(x))
Exemplo:
Entrada: ['testeee', 'ururguau', 'disculpa qq coisa!', "Vc eh muitooooo legal", "Oii, To trabahlando hj"]

Sa√≠das: ['teste', 'uruguai', 'desculpa qualquer coisa', 'voc√™ √© muito legal', 'oii to trabalhando hoje']

5. Substitui√ß√£o de Emojis
Neste passo, o c√≥digo realiza a substitui√ß√£o de emojis por r√≥tulos espec√≠ficos (incluindo o novo 'XD'). Tamb√©m utilizamos a biblioteca demoji para substituir emojis Unicode por suas descri√ß√µes.

Python

texto = texto.apply(substitui_emoji)
Exemplo:
Entrada: ['üòÄ', 'üòã', ':)', ':(', 'XD', 'ü§¢', "üò∫", "üéÇ"]

Sa√≠das: ['grinningface', 'facesavoringfood', 'emojipositivo', 'emojinegativo', 'emojipositivo', 'nauseatedface', 'grinningcat', 'birthdaycake']

6. Revers√£o da Substitui√ß√£o de V√≠rgulas
Ap√≥s a normaliza√ß√£o, o c√≥digo reverter√° a substitui√ß√£o anterior de v√≠rgulas por 'chavevirg', restaurando-as ao seu estado original.

Python

texto = texto.str.replace(r'chavevirg', ',')
7. Convers√£o para ASCII
Converte caracteres especiais para ASCII.

Python

texto = texto.apply(lambda x: unidecode(x))
8. Remo√ß√£o de Pontua√ß√µes e Caracteres Especiais
Este passo envolve a remo√ß√£o de pontua√ß√µes e caracteres especiais do texto, exceto quando esses caracteres s√£o parte de hashtags, datas ou n√∫meros com v√≠rgulas.

Python

texto = texto.apply(lambda x: re.sub(r'(?<!\d),(?=\D)|(?<=\D),(?!\d)|(?<!\d),(?=\d)|(?<!\d)\/|\/(?!\d)|_|[^\w#\/\s,\@]','', x))
9. Tokeniza√ß√£o e Remo√ß√£o de Stopwords (Novo)
Se tokenizar_texto=True, o texto passa pela fun√ß√£o tokenizar_e_limpar_texto. Isso envolve:

Converter o texto para min√∫sculas.

Tokenizar (dividir) o texto em palavras individuais.

Remover tokens que n√£o s√£o puramente alfab√©ticos.

Remover stopwords em portugu√™s (ex: 'de', 'a', 'o', 'que', 'e').

O resultado final na coluna texto_preprocessado ser√° uma lista de tokens (palavras), e n√£o mais uma string cont√≠nua.

Python

if tokenizar_texto:
    texto = texto.apply(tokenizar_e_limpar_texto)
Exemplo:
Entrada (ap√≥s etapas anteriores): 'ola tudo bem com voce eu estou otimo'

Sa√≠da: ['ola', 'tudo', 'bem', 'voce', 'eu', 'estou', 'otimo'] (Nota: 'com' seria removido se estivesse na lista de stopwords do NLTK).

Testes
<img src="https://github.com/ciberdem/ProjetoHEIWA-FAPESP/blob/main/CuradoriaExtracaoDados/pipeline_v2/assets/ex_teste_plv2.png" alt="Exemplo de sa√≠da de execu√ß√£o do c√≥digo">
